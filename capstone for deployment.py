# -*- coding: utf-8 -*-
"""Capstone (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ri10wkO3sILHRdf-A-qR82iPGBmWB4i
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('C:/Users/ishik/Downloads/Ishika Capstone/Ishika Capstone/Churn_Modelling.csv')

df.head()

df.shape

"""The dataset contains 10,000 entries and 14 columns."""

df.info()

df.isnull().sum()

"""There are no null values."""

df.describe()

df2= df.drop(columns=["RowNumber","CustomerId","Surname"])

"""The columns "RowNumber", "CustomerId", and "Surname" were dropped from the dataset because they are unique identifiers that do not provide meaningful insights for analysis. These columns do not contribute to understanding customer behavior or predicting churn. Removing them simplifies the dataset, focusing on relevant features."""

df2.head()

# boxplot to find out outliers
fig, ax = plt.subplots(figsize=(40, 40))

# Plot the box plots for all columns
df2.boxplot(ax=ax)

# Show the plot
plt.show()

"""Statistical Analysis: The variables like CreditScore, Age, Tenure, NumOfProducts has a narrow IQR,  with outliers detected at both ends of the distributions. Balance variable has a broader IQR, indicating significant variability among the customer balances. Many customers have zero balance and there are substantial outliers at the higher end, suggests that few customers have large balances. Likewise EstimatedSalary has a broader IQR, which indicate wide variability in customer salaries.

**Business Perspective to handle outliers:**

1. **CreditScore:** It is typically ranged between 300 and 850. Outliers at the lower end (below 400) might represent high-risk customers, while scores above 800 indicate very low-risk customers. These extremes can be important for decision-making in lending or risk management rather than being treated as statistical anomalies.

2. **Age:** While ages above 60 or below 20 might be statistical outliers, from a business perspective, they can represent important segments of the customer base, such as retirees or young adults. These age groups could have different financial needs and behaviors.

3. **Tenure:** Customers with very short or very long tenures could be outliers statistically, but they represent new customers or loyal, long-term customers, respectively. Understanding these groups is crucial for customer retention strategies.

4. **Balance:** Extremely high balances might be outliers statistically, but from a business perspective, these customers could be very valuable. Similarly, customers with zero balance could represent a risk of churn or different account usage patterns that are important to understand.

5. **NumOfProducts:** Customers with a very high number of products might be rare, but they represent highly engaged customers. This information is valuable for cross-selling and customer relationship management.

6. **EstimatedSalary:** High salaries might be statistical outliers, but these customers could be key targets for premium services. Similarly, customers with very low salaries might need different financial products and services.
"""

categorical_vars = ['Geography', 'Gender', 'Tenure','NumOfProducts', 'HasCrCard','IsActiveMember']
continuous_vars =  ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']

# Set up the figure with subplots
fig, axes = plt.subplots(len(continuous_vars), 2, figsize=(12, len(continuous_vars) * 4))

for i, var in enumerate(continuous_vars):
    # Histogram
    sns.histplot(data=df2, x=var, hue='Exited', ax=axes[i, 0], kde=True)
    axes[i, 0].set_title(f'Histogram of {var}')

    # Boxplot
    sns.boxplot(data=df2, x='Exited', y=var, ax=axes[i, 1])
    axes[i, 1].set_title(f'Boxplot of {var}')

plt.tight_layout()
plt.show()

"""Histogram and Boxplot of CreditScore
Histogram:

The distribution of credit scores shows a peak around 600-700.
Customers who have exited (Exited = 1) generally have lower credit scores compared to those who have not exited (Exited = 0).

Boxplot:

The median credit score is slightly higher for customers who stayed.
There are several outliers on the lower end for both exited and non-exited customers, but more pronounced for exited customers.

Histogram and Boxplot of Age
Histogram:

The age distribution peaks around 30-40 years.
A significant proportion of customers who exited are older (above 50 years), suggesting age might influence churn.

Boxplot:

The median age is higher for customers who exited.
There are many outliers in the age distribution for both exited and non-exited customers, especially among older age groups.

Histogram and Boxplot of Balance
Histogram:

Many customers have a balance close to zero.
Customers with higher balances (above 100,000) are more likely to have exited.

Boxplot:

The median balance is higher for customers who exited, but the overall distribution is similar for both groups.
There are several high-value outliers in both groups, indicating significant variability in account balances.

Histogram and Boxplot of EstimatedSalary
Histogram:

The estimated salary is uniformly distributed across the range.
The exit rate does not appear to vary significantly across different salary ranges, indicating salary might not be a strong predictor of churn.

Boxplot:

The median estimated salary is almost identical for both exited and non-exited customers.
There are no significant outliers in the salary distribution, and the interquartile ranges are similar for both groups.

CreditScore: Lower credit scores are associated with a higher likelihood
of churn.

Age: Older customers tend to have a higher churn rate.

Balance: Higher balances are associated with a higher likelihood of churn.

EstimatedSalary: Salary does not significantly impact churn, as the distribution is uniform across exited and non-exited groups.
"""

# Set up the figure with subplots
fig, axes = plt.subplots(3, 2, figsize=(12, 12))

for i, var in enumerate(categorical_vars):
    row, col = divmod(i, 2)  # Determine the row and column position
    sns.countplot(data=df2, x=var, hue='Exited', ax=axes[row, col])
    axes[row, col].set_title(f'Countplot of {var}')

plt.tight_layout()
plt.show()

"""**Countplot of Geography**

France: Most customers are from France, with a relatively smaller proportion exiting.

Spain: Fewer customers compared to France, but the exit rate is similar.

Germany: Has fewer customers than France but a higher proportion of exits, indicating a higher churn rate.

**Countplot of Gender**

Female: The count of female customers is lower than male customers, but the proportion of exits is higher.

Male: More male customers overall, with a lower proportion exiting compared to females.

**Countplot of Tenure**

Tenure: The distribution of tenure is fairly even, but exits are higher for customers with very short (0-1 years) and very long (9-10 years) tenures. This suggests that new customers and those with the longest relationships are more likely to churn.

**Countplot of NumOfProducts**

1 Product: The majority of customers have only one product, with a notable number exiting.

2 Products: A smaller number of customers have two products, but the exit rate is also significant.

3 or 4 Products: Very few customers have three or four products, with a low exit rate, indicating high engagement and lower churn.

**Countplot of HasCrCard**

No Credit Card: Customers without a credit card have a higher exit rate compared to those with a credit card.

Has Credit Card: The majority of customers have a credit card, and their exit rate is lower.

**Countplot of IsActiveMember**

Not Active: Customers who are not active members have a higher exit rate.

Active Member: Active members are less likely to exit, indicating engagement is crucial for retention.

Geography: German customers have a higher churn rate, suggesting that region-specific factors might influence customer retention.

Gender: Female customers are more likely to churn than male customers.

Tenure: Both very new and long-term customers show higher churn rates, possibly indicating onboarding issues and changing needs over time.

NumOfProducts: Customers with only one product are more likely to churn, while those with more products are more engaged and less likely to leave.

HasCrCard: Having a credit card is associated with lower churn, possibly due to higher engagement or satisfaction.

IsActiveMember: Active members are less likely to churn, highlighting the importance of customer engagement.
"""

#Count people having 0 balance in their accounts.
zero_balance_count = df2[df2['Balance'] == 0].shape[0]
total_customers = df2.shape[0]
zero_balance_percentage = (zero_balance_count / total_customers) * 100

print(f"Number of customers with zero balance: {zero_balance_count}")
print(f"Percentage of customers with zero balance: {zero_balance_percentage:.2f}%")

"""There are 36.17% of customers that have 0 balance in their account."""

import matplotlib.pyplot as plt
import seaborn as sns

# Data for visualization
balance_counts = pd.Series([zero_balance_count, total_customers - zero_balance_count], index=['Zero Balance', 'Non-Zero Balance'])

# Plotting the bar plot
plt.figure(figsize=(8, 6))
sns.barplot(x=balance_counts.index, y=balance_counts.values)
plt.title('Number of Customers with Zero vs Non-Zero Balance')
plt.ylabel('Number of Customers')
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Select the numerical features
numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary', 'Exited']

# Calculate the correlation matrix
correlation_matrix = df2[numerical_features].corr()

# Set the aesthetic style of the plots
sns.set_style("whitegrid")

# Create a mask to display only the lower triangle of the matrix
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))

# Create a figure and axis
plt.figure(figsize=(8, 4))

# Create a heatmap of the correlation matrix with the mask
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm_r', fmt=".2f", mask=mask)

# Set the title
plt.title('Correlation Matrix')

# Show the plot
plt.show()

"""Analyzing the correlation matrix, we can make the following conclusions:

Age and Exited (0.29): There is a weak positive correlation between a customer's age and the likelihood of churn. This suggests that older customers may be slightly more likely to churn compared to younger customers.

Balance and NumOfProducts (-0.30): There is a moderate negative correlation between the number of products a customer has and their account balance. This indicates that customers with more products tend to have lower account balances.

Balance and Exited (0.12): There is a weak positive correlation between a customer's account balance and the likelihood of churn. This suggests that customers with higher account balances may be slightly more likely to churn.

IsActiveMember and Exited (-0.16): There is a weak negative correlation between whether a customer is an active member and the likelihood of churn. This indicates that inactive members may be slightly more likely to churn.

Remaining correlations close to 0: The remaining correlations being close to 0 indicate that there is no correlation between them. This does not mean that there is no relationship at all, as there could still be nonlinear relationships or interactions between variables that are not captured by the correlation coefficient.
"""

# Define the features and target variable
X = df2.drop(['Exited'], axis=1)
y = df2['Exited']

# Unique value counts for each column
X.info()

"""Handling Categorical Variables: Encode categorical features like Geography and Gender."""

X = pd.get_dummies(X, columns=['Geography', 'Gender'], drop_first=True)

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Select the numerical columns for scaling
numerical_features = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']

# Apply scaling
X[numerical_features] = scaler.fit_transform(X[numerical_features])

z_scores = np.abs((X[numerical_features] - X[numerical_features].mean()) / X[numerical_features].std())

outliers_2_std = (z_scores > 2).sum()
print("Outliers beyond 2 standard deviations:")
print(outliers_2_std)

outliers_3_std = (z_scores > 3).sum()
print("Outliers beyond 3 standard deviations:")
print(outliers_3_std)

plt.figure(figsize=(10, 6))
sns.histplot(df2['Age'], bins=30, kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df2['NumOfProducts'], bins=10, kde=True)
plt.title('Number of Products Distribution')
plt.xlabel('Number of Products')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df2['CreditScore'], bins=30, kde=True)
plt.title('Credit Score Distribution')
plt.xlabel('Credit Score')
plt.ylabel('Frequency')
plt.show()

"""Age Distribution:

The histogram shows that the majority of customers fall within the age range of 20 to 60, with a peak around the age of 40. There are fewer customers beyond the age of 60.
Outliers beyond 3 standard deviations (133 customers) are significantly older or younger compared to the majority. These outliers might include very young customers or very senior customers.
Number of Products Distribution:

The distribution shows that most customers have 1 or 2 products. There are fewer customers with 3 or more products.
Outliers beyond 3 standard deviations (60 customers) likely have an unusually high number of products, which might represent a very small segment of highly engaged or affluent customers.
Credit Score Distribution:

The credit score distribution is slightly skewed towards higher scores, with most customers having scores between 500 and 800. A few customers have scores beyond 800 or below 400.
Outliers beyond 3 standard deviations (8 customers) include those with very high or very low credit scores.
Business Perspective on Outliers:
Age:

Older Customers: Customers who are significantly older may represent a valuable segment due to their potentially higher financial stability and loyalty. Removing these outliers would disregard an important customer base.
Younger Customers: Very young customers may represent new market entrants or future long-term customers. They might have different banking needs and preferences that could provide insights into product offerings tailored to younger demographics.
Number of Products:

High Number of Products: Customers with a high number of products are typically more engaged and valuable to the bank. They might be receiving tailored services or packages that encourage them to adopt multiple products. Removing these outliers would ignore a critical segment that demonstrates high engagement and potential profitability.
Credit Score:

High Credit Scores: Customers with exceptionally high credit scores often represent low risk and high profitability for the bank. They might be eligible for premium services and products.
Low Credit Scores: Customers with very low credit scores might be at higher risk but could also be targeted with specialized financial products aimed at credit rebuilding. Removing these outliers would eliminate insights into how to manage high-risk customers and leverage opportunities for offering tailored financial solutions.
"""

# Plot histogram to check for class imbalance in the 'Exited' column
plt.figure(figsize=(8, 6))
sns.countplot(x=y)
plt.title('Distribution of Churn (Exited) Classes')
plt.xlabel('Exited')
plt.ylabel('Count')
plt.xticks([0, 1], ['Not Churned', 'Churned'])
plt.show()

# Print the counts of each class to check for imbalance
class_counts = y.value_counts()
print("Class Counts:")
print(class_counts)
print("\nClass Percentages:")
print(class_counts / len(y) * 100)

"""The dataset does not exhibit class imbalance based on 10% criterion. Both classes are well-represented, allowing for robust model training and evaluation."""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Display the shapes of the resulting datasets
print("Training set shape: X_train =", X_train.shape, ", y_train =", y_train.shape)
print("Testing set shape: X_test =", X_test.shape, ", y_test =", y_test.shape)

from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn.metrics import roc_curve, auc

!pip install mord
from mord import LogisticIT
import matplotlib.pylab as plt
import seaborn as sns

!pip install dmba
from dmba import classificationSummary, gainsChart, liftChart
from dmba.metric import AIC_score

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

#Full Logistic Regression
param_grid_logistic_regression = {
    'C': [0.01, 0.1, 1, 10, 100],
    'solver': ['newton-cg', 'lbfgs']
}

logistic_regression = LogisticRegression(max_iter = 1000, random_state = 42)

grid_search_logistic_regression = GridSearchCV(logistic_regression, param_grid_logistic_regression, cv = 5, scoring='accuracy')

# Fit the model on the training data to discover the best hyperparameters

grid_search_logistic_regression.fit(X_train, y_train)

print("Optimal parameters for Logistic Regression:", grid_search_logistic_regression.best_params_)
print("Highest cross-validation accuracy for Logistic Regression:", grid_search_logistic_regression.best_score_ * 100)

best_logistic_regression = grid_search_logistic_regression.best_estimator_
best_logistic_regression.fit(X_train, y_train)

#Make predictions on the test set
pred_y_logistic_regression = best_logistic_regression.predict(X_test)
pred_y_proba_logistic_regression = best_logistic_regression.predict_proba(X_test)[:, 1]

#Make predictions on the training set
pred_y_logistic_regression_train = best_logistic_regression.predict(X_train)
pred_y_proba_logistic_regression_train = best_logistic_regression.predict_proba(X_train)[:, 1]

#classification report for the test set
print("Full Logistic Regression")
print("Confusion Matrix:")
print(confusion_matrix(y_test, pred_y_logistic_regression))
print("\nClassification Report:")
print(classification_report(y_test, pred_y_logistic_regression))
print("\nROC-AUC Score:", roc_auc_score(y_test, pred_y_proba_logistic_regression))

#classification report for the training set
print("Training Set Evaluation")
print("Confusion Matrix:")
print(confusion_matrix(y_train, pred_y_logistic_regression_train))
print("\nClassification Report:")
print(classification_report(y_train, pred_y_logistic_regression_train))
print("\nROC-AUC Score:", roc_auc_score(y_train, pred_y_proba_logistic_regression_train))

fpr, tpr, _ = roc_curve(y_test, pred_y_proba_logistic_regression)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

#Forward Regression
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

sfs_forward = SequentialFeatureSelector(logistic_regression, n_features_to_select = 10, direction = 'forward')
sfs_forward.fit(X_train, y_train)

selected_features_forward = sfs_forward.get_support(indices=True)

train_X_selected_forward = X_train.iloc[:, selected_features_forward]

test_X_selected_forward = X_test.iloc[:, selected_features_forward]

grid_search_logistic_regression_forward = GridSearchCV(logistic_regression, param_grid_logistic_regression, cv = 5, scoring = 'accuracy')

grid_search_logistic_regression_forward.fit(train_X_selected_forward, y_train)

print("Optimal parameters for Forward Logistic Regression:", grid_search_logistic_regression_forward.best_params_)
print("Highest cross-validation accuracy for Forward Logistic Regression:", grid_search_logistic_regression_forward.best_score_ * 100)

best_logistic_regression_forward = grid_search_logistic_regression_forward.best_estimator_
best_logistic_regression_forward.fit(train_X_selected_forward, y_train)

#Make predictions on the test set
pred_y_logistic_regression_forward = best_logistic_regression_forward.predict(test_X_selected_forward)
pred_y_proba_logistic_regression_forward = best_logistic_regression_forward.predict_proba(test_X_selected_forward)[:, 1]

#Make predictions on training set
pred_y_logistic_regression_forward_train = best_logistic_regression_forward.predict(train_X_selected_forward)
pred_y_proba_logistic_regression_forward_train = best_logistic_regression_forward.predict_proba(train_X_selected_forward)[:, 1]

#Evaluate the model on the test set
print("Forward Selection Logistic Regression")
print("Confusion Matrix:")
print(confusion_matrix(y_test, pred_y_logistic_regression_forward))
print("\nClassification Report:")
print(classification_report(y_test, pred_y_logistic_regression_forward))
print("\nROC-AUC Score:", roc_auc_score(y_test, pred_y_proba_logistic_regression_forward))

#evaluate the model on the training set
print("Forward Selection Logistic Regression - Training Set")
print("Confusion Matrix:")
print(confusion_matrix(y_train, pred_y_logistic_regression_forward_train))
print("\nClassification Report:")
print(classification_report(y_train, pred_y_logistic_regression_forward_train))
print("\nROC-AUC Score:", roc_auc_score(y_train, pred_y_proba_logistic_regression_forward_train))

fpr, tpr, _ = roc_curve(y_test, pred_y_proba_logistic_regression_forward)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

#Backward Regression
sfs_backward = SequentialFeatureSelector(logistic_regression, n_features_to_select = 10, direction = 'backward')
sfs_backward.fit(X_train, y_train)

selected_features_backward = sfs_backward.get_support(indices=True)
train_X_selected_backward = X_train.iloc[:, selected_features_backward]
test_X_selected_backward = X_test.iloc[:, selected_features_backward]

grid_search_logistic_regression_backward = GridSearchCV(logistic_regression, param_grid_logistic_regression, cv = 5, scoring = 'accuracy')
grid_search_logistic_regression_backward.fit(train_X_selected_backward, y_train)

print("Optimal parameters for Backward Logistic Regression:", grid_search_logistic_regression_backward.best_params_)
print("Highest cross-validation accuracy for Backward Logistic Regression:", grid_search_logistic_regression_backward.best_score_ * 100)

best_logistic_regression_backward = grid_search_logistic_regression_backward.best_estimator_
best_logistic_regression_backward.fit(train_X_selected_backward, y_train)

#Make predictions on test set
pred_y_logistic_regression_backward = best_logistic_regression_backward.predict(test_X_selected_backward)
pred_y_proba_logistic_regression_backward = best_logistic_regression_backward.predict_proba(test_X_selected_backward)[:, 1]

#Make predictions on training set
pred_y_logistic_regression_backward_train = best_logistic_regression_backward.predict(train_X_selected_backward)
pred_y_proba_logistic_regression_backward_train = best_logistic_regression_backward.predict_proba(train_X_selected_backward)[:, 1]

#Classification report for the test set
print("Backward Selection Logistic Regression")
print("Confusion Matrix:")
print(confusion_matrix(y_test, pred_y_logistic_regression_backward))
print("\nClassification Report:")
print(classification_report(y_test, pred_y_logistic_regression_backward))
print("\nROC-AUC Score:", roc_auc_score(y_test, pred_y_proba_logistic_regression_backward))

#classification report on training set
print("Backward Selection Logistic Regression - Training Set")
print("Confusion Matrix:")
print(confusion_matrix(y_train, pred_y_logistic_regression_backward_train))
print("\nClassification Report:")
print(classification_report(y_train, pred_y_logistic_regression_backward_train))
print("\nROC-AUC Score:", roc_auc_score(y_train, pred_y_proba_logistic_regression_backward_train))

fpr, tpr, _ = roc_curve(y_test, pred_y_proba_logistic_regression_backward)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Backward Logistic Regression')
plt.legend(loc="lower right")
plt.show()

#Stepwise Regression
sfs_stepwise = SequentialFeatureSelector(
    logistic_regression,
    n_features_to_select=10,
    direction='forward',
    tol=1e-3,
    cv=5
)
sfs_stepwise.fit(X_train, y_train)

selected_features_stepwise = sfs_stepwise.get_support(indices=True)
train_X_selected_stepwise = X_train.iloc[:, selected_features_stepwise]
test_X_selected_stepwise = X_test.iloc[:, selected_features_stepwise]

grid_search_logistic_regression_stepwise = GridSearchCV(
    logistic_regression,
    param_grid_logistic_regression,
    cv=5,
    scoring='accuracy'
)
grid_search_logistic_regression_stepwise.fit(train_X_selected_stepwise, y_train)

print("Optimal parameters for Stepwise Logistic Regression:", grid_search_logistic_regression_stepwise.best_params_)
print("Highest cross-validation accuracy for Stepwise Logistic Regression:", grid_search_logistic_regression_stepwise.best_score_ * 100)

best_logistic_regression_stepwise = grid_search_logistic_regression_stepwise.best_estimator_
best_logistic_regression_stepwise.fit(train_X_selected_stepwise, y_train)

#Make predictions on the test set
pred_y_logistic_regression_stepwise = best_logistic_regression_stepwise.predict(test_X_selected_stepwise)
pred_y_proba_logistic_regression_stepwise = best_logistic_regression_stepwise.predict_proba(test_X_selected_stepwise)[:, 1]

# Make predictions on the training set
pred_y_logistic_regression_stepwise_train = best_logistic_regression_stepwise.predict(train_X_selected_stepwise)
pred_y_proba_logistic_regression_stepwise_train = best_logistic_regression_stepwise.predict_proba(train_X_selected_stepwise)[:, 1]

#classification report for the test set
print("Stepwise Selection Logistic Regression")
print("Confusion Matrix:")
print(confusion_matrix(y_test, pred_y_logistic_regression_stepwise))
print("\nClassification Report:")
print(classification_report(y_test, pred_y_logistic_regression_stepwise))
print("\nROC-AUC Score:", roc_auc_score(y_test, pred_y_proba_logistic_regression_stepwise))

#classification report on training set
print("Stepwise Selection Logistic Regression - Training Set")
print("Confusion Matrix:")
print(confusion_matrix(y_train, pred_y_logistic_regression_stepwise_train))
print("\nClassification Report:")
print(classification_report(y_train, pred_y_logistic_regression_stepwise_train))
print("\nROC-AUC Score:", roc_auc_score(y_train, pred_y_proba_logistic_regression_stepwise_train))

fpr, tpr, _ = roc_curve(y_test, pred_y_proba_logistic_regression_stepwise)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - Stepwise Logistic Regression')
plt.legend(loc="lower right")
plt.show()

#Decision Tree
from sklearn.tree import DecisionTreeClassifier

param_grid_dt = {
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 10],
    'min_samples_leaf': [1, 4]
}

dtree = DecisionTreeClassifier(random_state=42)
grid_search_dt = GridSearchCV(dtree, param_grid_dt, cv=5, scoring='accuracy')
grid_search_dt.fit(X_train, y_train)

print("Best parameters for Decision Tree:", grid_search_dt.best_params_)
print("Best cross-validation accuracy for Decision Tree:", grid_search_dt.best_score_)

"""The model achieves a cross-validation accuracy of approximately 85.16%."""

# Train the Decision Tree model with best parameters
best_dtree = grid_search_dt.best_estimator_
best_dtree.fit(X_train, y_train)

from sklearn import tree

features = X.columns.tolist()

from sklearn.tree import DecisionTreeClassifier, plot_tree

plt.figure(figsize=(20, 10))
plot_tree(best_dtree, max_depth=3, feature_names=features, class_names=['Not Churned', 'Churned'], filled=True, rounded=True, fontsize=10)
plt.show()

# Make prediction on test set
y_pred_dt = best_dtree.predict(X_test)
y_pred_proba_dt = best_dtree.predict_proba(X_test)[:, 1]

# Make prediction on training set
y_pred_dt_train = best_dtree.predict(X_train)
y_pred_proba_dt_train = best_dtree.predict_proba(X_train)[:, 1]

#classification report for the test set
print("Decision Tree")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_dt))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_dt))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_pred_proba_dt))

#classification report for the training set
print("Decision Tree - Training Set")
print("Confusion Matrix:")
print(confusion_matrix(y_train, y_pred_dt_train))
print("\nClassification Report:")
print(classification_report(y_train, y_pred_dt_train))
print("\nROC-AUC Score:", roc_auc_score(y_train, y_pred_proba_dt_train))

"""True Negatives (TN): 2308. The model correctly predicted that 2308 customers did not churn.

False Positives (FP): 108. The model incorrectly predicted that 108 customers churned when they actually did not.

False Negatives (FN): 309. The model incorrectly predicted that 309 customers did not churn when they actually did.

True Positives (TP): 275. The model correctly predicted that 275 customers churned.

The model achieved a high precision of 0.88 and a recall of 0.96 for the non-churned class, translating to an F1-score of 0.92. This high precision and recall suggest that the model can reliably predict customers who will not churn, which minimizes unnecessary retention efforts and allows the company to focus resources effectively.

For the churned class, the model demonstrated a precision of 0.72 and a recall of 0.47, resulting in an F1-score of 0.57. While the precision indicates a reasonable ability to identify customers at risk of churning, the lower recall suggests that the model misses a significant portion of churned customers.

The overall accuracy of the model is 0.86, indicating that it correctly predicts the class of 86% of customers. The weighted average precision, recall, and F1-score further confirm the model's good performance across both classes, with scores of 0.85, 0.86, and 0.85, respectively. Additionally, the ROC-AUC score of 0.834 underscores the model's strong discriminatory ability to differentiate between churned and non-churned customers.

From a business perspective, these results are promising. The model's high accuracy and good ROC-AUC score suggest it is a reliable tool for predicting customer churn. However, the moderate recall for the churned class highlights the need for further refinement to better capture at-risk customers, ultimately aiding in the development of more effective retention strategies.
"""

# Plot ROC Curve
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)
plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_score(y_test, y_pred_proba_dt):.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()

"""The ROC curve shows that the Decision Tree model has a good true positive rate for a range of false positive rates.

For example, at a false positive rate of 0.2, the true positive rate is approximately 0.8. This means that when the model incorrectly identifies 20% of negatives as positives, it correctly identifies 80% of positives.

Customer Churn Prediction:
The ROC curve and AUC score provide a clear indication that the Decision Tree model is effective at predicting customer churn. With an AUC of 0.83, the model can be used to identify customers who are at risk of churning with a reasonable degree of accuracy.
"""

#Random Forest Model
from sklearn.ensemble import RandomForestClassifier

param_grid_rf = {
    'n_estimators': [50,100,200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rforest = RandomForestClassifier(random_state=42)
grid_search_rf = GridSearchCV(rforest, param_grid_rf, cv=5, scoring='accuracy')
grid_search_rf.fit(X_train, y_train)

print("Best parameters for Random Forest:", grid_search_rf.best_params_)
print("Best cross-validation accuracy for Random Forest:", grid_search_rf.best_score_)

best_rf = grid_search_rf.best_estimator_
best_rf.fit(X_train, y_train)

importances = best_rf.feature_importances_
feature_names = X.columns

print(importances)

"""This represents the importance of each feature in the dataset. Each value corresponds to the importance of the respective feature in predicting the target variable."""

# Create a DataFrame for visualization
feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest')
plt.gca().invert_yaxis()  # To display the highest importance at the top
plt.show()

"""Age has an importance of 0.29529131, it is a highly influential feature in the model compared to HasCrCard with an importance of 0.01209225.

The bar chart titled "Feature Importance in Random Forest" visually represents the importance of each feature used in the Random Forest model for predicting customer churn. The importance of a feature indicates how much weight the model places on that feature when making its predictions.

Age:

This is the most important feature in the model, with an importance score of around 0.295. This implies that the age of the customers has the most significant influence on predicting whether they will churn or not.

NumOfProducts:

The number of products a customer has is the second most important feature, with a score of about 0.192. This suggests that customers with fewer products are more likely to churn.

Balance:

The balance in a customer's account is the third most important feature, with an importance score of approximately 0.124. Lower balances are associated with a higher likelihood of churn.

EstimatedSalary:

This feature also plays a significant role, with an importance score of around 0.095. It indicates that the estimated salary of a customer affects their likelihood of staying or leaving.

CreditScore:

Credit score has a notable impact with an importance score of about 0.093. Customers with lower credit scores are more likely to churn.

IsActiveMember:

Being an active member has an importance score of around 0.058, showing that active members are less likely to churn.

Tenure:

The duration of time a customer has been with the bank (tenure) has a lower importance score of approximately 0.057, but it still plays a role in predicting churn.

Geography_Germany:

This feature represents customers located in Germany and has an importance score of around 0.038, indicating geographical location can influence churn.

Gender_Male:

Gender (specifically being male) has a smaller importance score of about 0.020, showing some influence on churn prediction.

HasCrCard:

Whether a customer has a credit card has a very low importance score of around 0.012, indicating minimal influence on churn.

Geography_Spain:

Similar to HasCrCard, this feature has a very low importance score, around 0.011, suggesting that being located in Spain has minimal impact on churn prediction.

Business Perspective:

From this analysis, it is clear that demographic factors like age and the number of products held by a customer are critical in predicting churn. Customers who are younger and have fewer products with the bank are more likely to leave. Additionally, financial metrics such as account balance and estimated salary also play significant roles. Active membership status and credit score are moderately important, while geographical location and whether the customer has a credit card have the least influence on churn predictions. These insights can guide the bank in designing targeted interventions to reduce churn by focusing on these key areas.
"""

# Make prediction on the test set
y_pred_rf = best_rf.predict(X_test)
y_pred_proba_rf = best_rf.predict_proba(X_test)[:, 1]

#Make prediction on the training set
y_pred_rf_train = best_rf.predict(X_train)
y_pred_proba_rf_train = best_rf.predict_proba(X_train)[:, 1]

#classification report for the test set
print("Random Forest")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_pred_proba_rf))

#classification report for the training set
print("Random Forest")
print("Confusion Matrix:")
print(confusion_matrix(y_train, y_pred_rf_train))
print("\nClassification Report:")
print(classification_report(y_train, y_pred_rf_train))
print("\nROC-AUC Score:", roc_auc_score(y_train, y_pred_proba_rf_train))

"""True Positives (TP): 266 These are the instances where the model correctly predicted the positive class (churn). This means the model correctly identified 266 customers who actually churned.

True Negatives (TN): 2339. These are the instances where the model correctly predicted the negative class (not churn). This means the model correctly identified 2339 customers who did not churn.

False Positives (FP): 77. These are the instances where the model incorrectly predicted the positive class (churn) when it was actually negative (not churn). This means the model incorrectly identified 77 customers as having churned when they did not.

False Negatives (FN): 318. These are the instances where the model incorrectly predicted the negative class (not churn) when it was actually positive (churn). This means the model incorrectly identified 318 customers as not having churned when they actually did.

From a business perspective:

The model correctly identified 2339 customers who did not churn and 266 customers who did churn.
There were 77 instances where the model falsely identified customers as churners (false positives). This could lead to unnecessary interventions or retention efforts directed at these customers.
There were 318 instances where the model failed to identify churners (false negatives). This means these customers might leave the bank without the bank taking any preemptive actions to retain them.

The classification report for the Random Forest model provides valuable insights into its performance on the test dataset. For the "Not Churned" class (class 0), the model achieves a high precision of 0.88, indicating that 88% of the instances predicted as "Not Churned" are correct. The recall for this class is even higher at 0.97, meaning the model correctly identifies 97% of all actual "Not Churned" instances. The F1-Score, which balances precision and recall, is 0.92, further demonstrating the model's strong performance in identifying customers who are not likely to churn.

In contrast, for the "Churned" class (class 1), the model's precision is 0.78, indicating that 78% of the instances predicted as "Churned" are correct. However, the recall for this class is significantly lower at 0.46, suggesting that the model correctly identifies only 46% of all actual "Churned" instances. The F1-Score for the "Churned" class is 0.57, reflecting the need for improvement in balancing precision and recall for identifying potential churners.

Overall, the model achieves an accuracy of 0.87, meaning that 87% of the total predictions are correct. The macro-average precision, recall, and F1-Score are 0.83, 0.71, and 0.75, respectively, while the weighted averages, considering the support of each class, are 0.86, 0.87, and 0.85. These metrics indicate that the model performs well across both classes, but there is a disparity in performance between identifying churned and non-churned customers.

The ROC-AUC score of 0.864 suggests that the model has a strong ability to distinguish between churners and non-churners, providing a reliable measure for decision-making. From a business perspective, the model is highly accurate in identifying customers who are not likely to churn, giving the bank confidence in predicting customer retention. However, the model's lower recall for identifying churners highlights the need to enhance its ability to detect potential churners accurately. Improving this aspect is crucial for the bank to take preemptive actions to retain at-risk customers effectively.
"""

# Plot ROC Curve
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_score(y_test, y_pred_proba_rf):.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()

"""The AUC (Area Under the Curve) score of 0.86 indicates a strong ability of the model to discriminate between the two classes. An AUC score of 0.86 suggests that there is an 86% chance that the model will correctly distinguish between a randomly chosen churned customer and a randomly chosen non-churned customer.

In practical terms, the closer the ROC curve is to the top left corner of the plot, the better the model's performance. The diagonal line represents a model with no discriminative ability, where the true positive rate equals the false positive rate (AUC = 0.5). Since the ROC curve for the Random Forest model is well above this diagonal line, it confirms the model's effectiveness.

From a business perspective, this high AUC score means that the Random Forest model is reliable for predicting customer churn. It can be effectively used to identify customers who are at risk of churning, allowing the bank to take proactive measures to retain these customers. The model's strong performance can aid in targeted marketing and personalized customer engagement strategies to improve customer retention.
"""

# Create a DataFrame to easily visualize and sort feature importances
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Remove the last three least important variables
selected_features = feature_importance_df['Feature'][:-3]

# Create a new DataFrame with the selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Define the Random Forest model and the parameter grid for GridSearchCV
rf_model = RandomForestClassifier(random_state=42)
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Perform GridSearchCV to find the best parameters
grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy')
grid_search_rf.fit(X_train_selected, y_train)

# Get the best Random Forest model
best_rf2 = grid_search_rf.best_estimator_

# Evaluate the model on the test set
y_pred_rf = best_rf2.predict(X_test_selected)
y_pred_proba_rf = best_rf2.predict_proba(X_test_selected)[:, 1]

# Evaluate the model on the training set
y_pred_rf_train = best_rf2.predict(X_train_selected)
y_pred_proba_rf_train = best_rf2.predict_proba(X_train_selected)[:, 1]

# Print the classification report and ROC-AUC score for the test set
print("Random Forest after removing least important features")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_pred_proba_rf))

# Print the classification report and ROC-AUC score for the training set
print("\nClassification report for the training set")
print("Confusion Matrix:")
print(confusion_matrix(y_train, y_pred_rf_train))
print("\nClassification Report:")
print(classification_report(y_train, y_pred_rf_train))
print("\nROC-AUC Score:", roc_auc_score(y_train, y_pred_proba_rf_train))

# Plot ROC Curve for the test set
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_score(y_test, y_pred_proba_rf):.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()

#Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier

param_grid_gbm = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

gbm = GradientBoostingClassifier(random_state=42)
grid_search_gbm = GridSearchCV(gbm, param_grid_gbm, cv=5, scoring='accuracy')
grid_search_gbm.fit(X_train, y_train)

print("Best parameters for Gradient Boosting Machines:", grid_search_gbm.best_params_)
print("Best cross-validation accuracy for Gradient Boosting Machines:", grid_search_gbm.best_score_)

"""The reported best cross-validation accuracy for Gradient Boosting Machines is approximately 0.8578, indicating the model's performance during the hyperparameter tuning process. This accuracy suggests that the model is likely to perform well on unseen data."""

# Train the Gradient Boosting Machines model with best parameters
best_gbm = grid_search_gbm.best_estimator_
best_gbm.fit(X_train, y_train)

# Make prediction for the test set
y_pred_gbm = best_gbm.predict(X_test)
y_pred_proba_gbm = best_gbm.predict_proba(X_test)[:, 1]

#Make prediction for the train set
y_pred_gbm_train = best_gbm.predict(X_train)
y_pred_proba_gbm_train = best_gbm.predict_proba(X_train)[:, 1]

#classification report for test set
print("Gradient Boosting Machines")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_gbm))
print("\nClassification Report:")
print(classification_report(y_test, y_pred_gbm))
print("\nROC-AUC Score:", roc_auc_score(y_test, y_pred_proba_gbm))

#Classification report for the train set
print("Gradient Boosting Machines")
print("Confusion Matrix:")
print(confusion_matrix(y_train, y_pred_gbm_train))
print("\nClassification Report:")
print(classification_report(y_train, y_pred_gbm_train))
print("\nROC-AUC Score:", roc_auc_score(y_train, y_pred_gbm_train))

"""True Positives (TP): The model correctly predicted 268 instances as positive (churned). These are the customers who actually churned and the model correctly identified them as churned.

True Negatives (TN): The model correctly predicted 2339 instances as negative (not churned). These are the customers who did not churn and the model correctly identified them as not churned.

False Positives (FP): The model incorrectly predicted 77 instances as positive (churned) when they were actually negative (not churned). These are customers who did not churn, but the model incorrectly flagged them as churned.

False Negatives (FN): The model incorrectly predicted 316 instances as negative (not churned) when they were actually positive (churned). These are customers who churned, but the model failed to identify them.

The classification report for the Gradient Boosting Machines (GBM) model provides a detailed performance evaluation. For the non-churned class (Class 0), the model demonstrates a high precision of 0.88, indicating that 88% of the customers predicted as not churned were indeed non-churned. The recall for this class is 0.97, meaning the model correctly identified 97% of the actual non-churned customers. This results in an F1-score of 0.92, reflecting a strong balance between precision and recall. The support for this class, which is the number of actual non-churned instances, is 2416.

For the churned class (Class 1), the model's precision is 0.78, showing that 78% of the customers predicted as churned were actually churned. However, the recall for this class is lower at 0.46, indicating that the model correctly identified only 46% of the actual churned customers. The F1-score for this class is 0.58, suggesting a moderate balance between precision and recall. The support for the churned class is 584.

Overall, the model achieves an accuracy of 0.87, meaning that 87% of the total predictions were correct. The macro average precision is 0.83, recall is 0.71, and F1-score is 0.75, representing the average performance across both classes. The weighted averages, which account for the number of true instances in each class, are 0.86 for precision, 0.87 for recall, and 0.86 for the F1-score.

The ROC-AUC score for the model is 0.861, indicating a good ability to distinguish between churned and non-churned customers. A score closer to 1 implies better performance. In summary, while the GBM model performs strongly overall with high precision and recall for the non-churned class, it faces challenges with recall for the churned class, indicating room for improvement in identifying customers at risk of churn. Nonetheless, the high accuracy and ROC-AUC score suggest the model is generally effective at predicting customer churn.
"""

# Plot ROC Curve
fpr_gbm, tpr_gbm, _ = roc_curve(y_test, y_pred_proba_gbm)
plt.plot(fpr_gbm, tpr_gbm, label=f'Gradient Boosting Machines (AUC = {roc_auc_score(y_test, y_pred_proba_gbm):.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()

"""The area under the curve (AUC) is 0.86, indicating that the model has a strong ability to discriminate between the two classes. An AUC value of 0.5 suggests no discrimination (random guessing), while an AUC value of 1.0 indicates perfect discrimination. Therefore, an AUC of 0.86 means the GBM model is quite effective at predicting customer churn.

The ROC curve shows a good true positive rate for a range of false positive rates. For example, at a false positive rate of 0.2, the true positive rate is approximately 0.7. This means that when the model incorrectly identifies 20% of negatives as positives, it correctly identifies 70% of positives. This balance between sensitivity and specificity highlights the GBM model's robustness in identifying customers who are at risk of churn while minimizing false alarms.
"""

# Get feature importances
importances = best_gbm.feature_importances_
feature_names = X.columns

# Create a DataFrame for visualization
feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
plt.barh(feature_importances['Feature'], feature_importances['Importance'])
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Gradient Boosting')
plt.gca().invert_yaxis()  # To display the highest importance at the top
plt.show()

"""Age is the most influential feature, with the highest importance score. This indicates that age plays a significant role in predicting whether a customer will churn.

Number of Products is the second most important feature. The number of products a customer has is a strong indicator of their likelihood to churn.

Balance also has a high importance score, suggesting that the account balance of a customer is crucial in determining their churn risk.

IsActiveMember follows closely, indicating that whether a customer is an active member significantly impacts churn predictions.

Estimated Salary and Credit Score have moderate importance, implying they are relevant but not as critical as the top features.

Geography_Germany and Tenure have lower importance scores but still contribute to the model's predictions.

Gender_Male, Geography_Spain, and HasCrCard have the least importance, indicating they have a minimal effect on the churn prediction.
"""

# Sort the feature importances to get the least important features
least_important_features = feature_importances.sort_values(by='Importance', ascending=True).head(3)['Feature'].tolist()

# Drop the least important features from the dataset
X_train_reduced = X_train.drop(columns=least_important_features)
X_test_reduced = X_test.drop(columns=least_important_features)

# Define the parameter grid for Gradient Boosting
param_grid_gbm_reduced = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

# Initialize the Gradient Boosting model
gbm_reduced = GradientBoostingClassifier(random_state=42)

# Perform Grid Search
grid_search_gbm_reduced = GridSearchCV(gbm_reduced, param_grid_gbm_reduced, cv=5, scoring='accuracy')
grid_search_gbm_reduced.fit(X_train_reduced, y_train)

best_gbm_reduced = grid_search_gbm_reduced.best_estimator_

# Train the best model
best_gbm_reduced.fit(X_train_reduced, y_train)

# Make predictions for the test set
y_pred_gbm_reduced = best_gbm_reduced.predict(X_test_reduced)
y_pred_proba_gbm_reduced = best_gbm_reduced.predict_proba(X_test_reduced)[:, 1]

# Make predictions for the training set
y_pred_gbm_train_reduced = best_gbm_reduced.predict(X_train_reduced)
y_pred_proba_gbm_train_reduced = best_gbm_reduced.predict_proba(X_train_reduced)[:, 1]

# Evaluate the model on the test set
print("Gradient Boosting Machines (Reduced Features)")
print("Confusion Matrix (Test Set):")
print(confusion_matrix(y_test, y_pred_gbm_reduced))
print("\nClassification Report (Test Set):")
print(classification_report(y_test, y_pred_gbm_reduced))
print("\nROC-AUC Score (Test Set):", roc_auc_score(y_test, y_pred_proba_gbm_reduced))

# Evaluate the model on the training set
print("\nConfusion Matrix (Training Set):")
print(confusion_matrix(y_train, y_pred_gbm_train_reduced))
print("\nClassification Report (Training Set):")
print(classification_report(y_train, y_pred_gbm_train_reduced))
print("\nROC-AUC Score (Training Set):", roc_auc_score(y_train, y_pred_proba_gbm_train_reduced))

# Plot the ROC curve
fpr_gbm_reduced, tpr_gbm_reduced, _ = roc_curve(y_test, y_pred_proba_gbm_reduced)
plt.plot(fpr_gbm_reduced, tpr_gbm_reduced, label=f'Gradient Boosting (Reduced Features) (AUC = {roc_auc_score(y_test, y_pred_proba_gbm_reduced):.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()

# Display the feature importances
print(feature_importances)

"""Based on the performance metrics, Random Forest and Gradient Boosting Machines are the top contenders. Both models have the same accuracy (0.87) and very close ROC-AUC scores (0.86 for both). However, Gradient Boosting Machines slightly outperforms Random Forest in terms of the F1-score for Class 1 (0.58 vs. 0.57), which indicates better performance in handling the minority class (churned customers)."""



from sklearn.preprocessing import StandardScaler
import joblib

scaler = StandardScaler()
scaler.fit(X_train[numerical_features])

# Save the scaler
joblib.dump(scaler, 'models/scaler.pkl')

# Save Logistic Regression Model
joblib.dump(best_logistic_regression, 'models/logistic_regression_model.pkl')

# Save Decision Tree Model
joblib.dump(best_dtree, 'models/decision_tree_model.pkl')

# Save Random Forest Model
joblib.dump(best_rf, 'models/random_forest_model.pkl')

# Save Gradient Boosting Model
joblib.dump(best_gbm, 'models/gradient_boosting_model.pkl')





